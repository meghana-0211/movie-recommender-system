{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH6MN478+UwuaU7ehIt2z3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eVpFk-W2EmYZ",
        "outputId": "8b6acc5f-4ad1-48d7-ebe6-372d1c31c822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (553 bytes)\n",
            "Collecting dgllife\n",
            "  Downloading dgllife-0.3.2-py3-none-any.whl.metadata (667 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Collecting torchdata>=0.5.0 (from dgl)\n",
            "  Downloading torchdata-0.9.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgllife) (2.2.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from dgllife) (0.2.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->dgllife) (3.5.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.5.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (3.1.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (3.0.2)\n",
            "Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dgllife-0.3.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.9.0-cp310-cp310-manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata, dgllife, dgl\n",
            "Successfully installed dgl-2.1.0 dgllife-0.3.2 torchdata-0.9.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.27.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Downloading neo4j-5.27.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j\n",
            "Successfully installed neo4j-5.27.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install dgl dgllife\n",
        "!pip install transformers\n",
        "!pip install neo4j\n",
        "!pip install networkx\n",
        "!pip install scikit-learn\n",
        "!pip install pandas numpy\n",
        "!pip install imblearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ0XUeWcGOVz",
        "outputId": "8c90f786-d575-43ec-afed-b5980db72f17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/bollywood_recommender/src\n",
        "!mkdir -p /content/bollywood_recommender/data\n"
      ],
      "metadata": {
        "id": "VMGoes4EGhyf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/config.py\n",
        "class Config:\n",
        "    # Data paths\n",
        "    RAW_DATA_PATH = '/content/bollywood_data_set.csv'\n",
        "    PROCESSED_DATA_PATH = '/content/bollywood_data_set.csv'\n",
        "\n",
        "    # Neo4j Database Configuration\n",
        "    NEO4J_URI = 'bolt://localhost:7687'\n",
        "    NEO4J_USERNAME = 'neo4j'\n",
        "    NEO4J_PASSWORD = '12345678'\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    EMBEDDING_DIM = 128\n",
        "    CNN_FILTER_SIZE = 3\n",
        "    GNN_LAYERS = 2\n",
        "\n",
        "    # Recommendation Parameters\n",
        "    TOP_K_RECOMMENDATIONS = 3\n",
        "    SIMILARITY_THRESHOLD = 0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_8V1vQBGqgu",
        "outputId": "9ca0c101-127f-443c-adb6-ae2a794eb3dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bollywood_recommender/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/src/data_preprocessing.py\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_data(self):\n",
        "        df = pd.read_csv(self.config.RAW_DATA_PATH)\n",
        "        return df\n",
        "\n",
        "    def _extract_year(self, text):\n",
        "        text = str(text)\n",
        "        # Try extracting year from parentheses\n",
        "        parentheses_match = re.search(r'\\((\\d{4})\\)', text)\n",
        "        if parentheses_match:\n",
        "            return int(parentheses_match.group(1))\n",
        "\n",
        "        # Try extracting year with hyphen\n",
        "        hyphen_match = re.search(r'-(\\d{4})', text)\n",
        "        if hyphen_match:\n",
        "            return int(hyphen_match.group(1))\n",
        "\n",
        "        # Try direct year match\n",
        "        direct_match = re.search(r'\\b(\\d{4})\\b', text)\n",
        "        if direct_match:\n",
        "            return int(direct_match.group(1))\n",
        "\n",
        "        return np.nan\n",
        "\n",
        "    def _encode_categorical(self, series, delimiter=None, top_n=50):\n",
        "        if delimiter:\n",
        "            series = series.str.split(delimiter)\n",
        "\n",
        "        all_categories = series.explode().value_counts()\n",
        "        top_categories = all_categories.head(top_n).index.tolist()\n",
        "        category_mapping = {cat: idx for idx, cat in enumerate(top_categories)}\n",
        "\n",
        "        def encode_entry(entry):\n",
        "            if isinstance(entry, list):\n",
        "                return [category_mapping.get(cat, -1) for cat in entry if cat in category_mapping]\n",
        "            else:\n",
        "                return category_mapping.get(entry, -1)\n",
        "\n",
        "        encoded = series.apply(encode_entry)\n",
        "        return encoded\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        # Clean column names\n",
        "        df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "        # Extract year from movie name\n",
        "        df['year_of_release'] = df['movie_name'].apply(self._extract_year)\n",
        "\n",
        "        # Clean and convert runtime\n",
        "        df['runtime'] = df['runtime'].astype(str)\n",
        "        df['runtime'] = df['runtime'].str.replace(' min', '')\n",
        "        df['runtime'] = pd.to_numeric(df['runtime'], errors='coerce')\n",
        "\n",
        "        # Clean votes\n",
        "        df['no_of_votes'] = df['no_of_votes'].astype(str).str.replace(',', '').str.strip()\n",
        "        df['no_of_votes'] = pd.to_numeric(df['no_of_votes'], errors='coerce')\n",
        "        df['no_of_votes'] = df['no_of_votes'].fillna(0).astype(int)\n",
        "\n",
        "        # Handle missing years\n",
        "        current_year = pd.Timestamp.now().year\n",
        "        df['year_of_release'] = df['year_of_release'].apply(\n",
        "            lambda x: x if pd.notnull(x) and 1900 <= x <= current_year else np.nan\n",
        "        )\n",
        "\n",
        "        median_year = df['year_of_release'].median()\n",
        "        df['year_of_release'] = df['year_of_release'].fillna(median_year)\n",
        "\n",
        "        # Define columns for processing\n",
        "        numeric_columns = ['year_of_release', 'runtime', 'imdb_rating', 'no_of_votes']\n",
        "        text_columns = ['movie_name', 'plot_description', 'director', 'actors']\n",
        "\n",
        "        # Process numeric columns\n",
        "        for col in numeric_columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        numeric_imputer = SimpleImputer(strategy='median')\n",
        "        df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])\n",
        "\n",
        "        # Process text columns\n",
        "        for col in text_columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "            df[col] = df[col].str.strip()\n",
        "\n",
        "        # Feature engineering\n",
        "        df['runtime_minutes'] = df['runtime'].astype(float)\n",
        "        df['directors_encoded'] = self._encode_categorical(df['director'])\n",
        "        df['actors_encoded'] = self._encode_categorical(df['actors'], delimiter='|')\n",
        "\n",
        "        # Normalize features\n",
        "        scaler = StandardScaler()\n",
        "        df[['normalized_rating', 'normalized_votes']] = scaler.fit_transform(\n",
        "            df[['imdb_rating', 'no_of_votes']]\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def balance_dataset(self, df):\n",
        "        X = df[['normalized_rating', 'normalized_votes']]\n",
        "        y = df['imdb_rating']\n",
        "\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "        balanced_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "        balanced_df['imdb_rating'] = y_resampled\n",
        "\n",
        "        return balanced_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjT_8AFvHD4Q",
        "outputId": "13225580-df9d-4809-8a53-68e5db88734e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bollywood_recommender/src/data_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/src/feature_extraction.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class TextFeatureExtractor:\n",
        "    def __init__(self, model_name='ai4bharat/indic-bert'):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def extract_text_features(self, texts):\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=input_dim,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYBIw2FiHItm",
        "outputId": "cc8c91ce-0d8c-4b1d-b915-9652d18020c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bollywood_recommender/src/feature_extraction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/src/graph_nn.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, graph, features):\n",
        "        graph = graph.local_var()\n",
        "        h = torch.matmul(features, self.W)\n",
        "        graph.ndata['h'] = h\n",
        "        graph.apply_edges(self.edge_attention)\n",
        "        graph.edata['a'] = F.softmax(graph.edata['a'], dim=1)\n",
        "        graph.edata['a'] = F.dropout(graph.edata['a'], self.dropout)\n",
        "        graph.update_all(fn.u_mul_e('h', 'a', 'm'), fn.sum('m', 'h'))\n",
        "        return graph.ndata['h']\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        concat_features = torch.cat([edges.src['h'], edges.dst['h']], dim=1)\n",
        "        edge_attention = self.leaky_relu(torch.matmul(concat_features, self.a))\n",
        "        return {'a': edge_attention}\n",
        "\n",
        "class MovieGraphNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
        "        super(MovieGraphNeuralNetwork, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(GraphAttentionLayer(input_dim, hidden_dim))\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.layers.append(GraphAttentionLayer(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, graph, features):\n",
        "        x = features\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(graph, x)\n",
        "            x = self.leaky_relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgZEeNLaHOQD",
        "outputId": "38c3310c-7da5-4668-a154-45df39c9fbdc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bollywood_recommender/src/graph_nn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/src/graph_construction.py\n",
        "from neo4j import GraphDatabase\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "class GraphConstructor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def create_movie_graph(self, movies_df):\n",
        "        G = nx.Graph()\n",
        "\n",
        "        for _, movie in movies_df.iterrows():\n",
        "            G.add_node(\n",
        "                movie['movie_name'],\n",
        "                type='movie',\n",
        "                rating=movie['imdb_rating'],\n",
        "                year=movie['year_of_release']\n",
        "            )\n",
        "\n",
        "        similarity_matrix = self._compute_movie_similarity(movies_df)\n",
        "\n",
        "        for i in range(len(movies_df)):\n",
        "            for j in range(i+1, len(movies_df)):\n",
        "                if similarity_matrix[i, j] > self.config.SIMILARITY_THRESHOLD:\n",
        "                    G.add_edge(\n",
        "                        movies_df.iloc[i]['movie_name'],\n",
        "                        movies_df.iloc[j]['movie_name'],\n",
        "                        weight=similarity_matrix[i, j]\n",
        "                    )\n",
        "\n",
        "        return G\n",
        "\n",
        "    def _compute_movie_similarity(self, movies_df):\n",
        "        features = movies_df[['normalized_rating', 'normalized_votes']].values\n",
        "        norm = np.linalg.norm(features, axis=1)\n",
        "        similarity = np.dot(features, features.T) / (norm[:, None] * norm[None, :])\n",
        "        return similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moThmuzvHRi-",
        "outputId": "576589be-74e8-4c8e-d249-41d80eb45d7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bollywood_recommender/src/graph_construction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/src/recommendation_engine.py\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class BollywoodRecommender:\n",
        "    def __init__(self, graph, feature_extractor):\n",
        "        self.graph = graph\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def get_recommendations(self, base_movie, top_k=3):\n",
        "        if base_movie not in self.graph.nodes:\n",
        "            raise ValueError(f\"Movie {base_movie} not found in graph\")\n",
        "\n",
        "        content_candidates = self._content_based_recommendation(base_movie)\n",
        "        graph_candidates = self._graph_based_recommendation(base_movie)\n",
        "        recommendations = self._merge_recommendations(\n",
        "            content_candidates,\n",
        "            graph_candidates,\n",
        "            top_k\n",
        "        )\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def _content_based_recommendation(self, base_movie):\n",
        "        candidates = {}\n",
        "        for movie in self.graph.nodes:\n",
        "            if movie != base_movie:\n",
        "                similarity = self._compute_content_similarity(base_movie, movie)\n",
        "                candidates[movie] = similarity\n",
        "\n",
        "        return sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    def _graph_based_recommendation(self, base_movie):\n",
        "        candidates = {}\n",
        "        for movie in nx.single_source_shortest_path_length(\n",
        "            self.graph, base_movie, cutoff=2\n",
        "        ).keys():\n",
        "            if movie != base_movie:\n",
        "                candidates[movie] = self.graph[base_movie][movie]['weight']\n",
        "\n",
        "        return sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    def _merge_recommendations(self, content_candidates, graph_candidates, top_k):\n",
        "        merged_candidates = {}\n",
        "\n",
        "        for movie, score in content_candidates:\n",
        "            merged_candidates[movie] = 0.6 * score\n",
        "\n",
        "        for movie, score in graph_candidates:\n",
        "            merged_candidates[movie] = merged_candidates.get(movie, 0) + 0.4 * score\n",
        "\n",
        "        top_recommendations = sorted(\n",
        "            merged_candidates.items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )[:top_k]\n",
        "\n",
        "        return [movie for movie, _ in top_recommendations]\n",
        "\n",
        "    def _compute_content_similarity(self, movie1, movie2):\n",
        "        return cosine_similarity(\n",
        "            self.feature_extractor.extract_text_features([movie1]),\n",
        "            self.feature_extractor.extract_text_features([movie2])\n",
        "        )[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q9ejp0vHSKb",
        "outputId": "6b89d9a9-aa8d-4a61-ab2e-7d6e2e05356b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bollywood_recommender/src/recommendation_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/src/rag_components.py\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "class RAGRetriever:\n",
        "    def __init__(self, config):\n",
        "        self.model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        self.neo4j_driver = GraphDatabase.driver(\n",
        "            config.NEO4J_URI,\n",
        "            auth=(config.NEO4J_USERNAME, config.NEO4J_PASSWORD)\n",
        "        )\n",
        "\n",
        "    def _get_embeddings(self, text):\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _create_graph_index(self, tx, movie_data):\n",
        "        # Create movie nodes with embeddings\n",
        "        tx.run(\"\"\"\n",
        "            UNWIND $movies as movie\n",
        "            CREATE (m:Movie {\n",
        "                title: movie.title,\n",
        "                plot: movie.plot,\n",
        "                embedding: movie.embedding,\n",
        "                year: movie.year,\n",
        "                rating: movie.rating\n",
        "            })\n",
        "        \"\"\", movies=movie_data)\n",
        "\n",
        "        # Create similarity relationships\n",
        "        tx.run(\"\"\"\n",
        "            MATCH (m1:Movie), (m2:Movie)\n",
        "            WHERE id(m1) < id(m2)\n",
        "            WITH m1, m2, gds.similarity.cosine(m1.embedding, m2.embedding) AS similarity\n",
        "            WHERE similarity > 0.7\n",
        "            CREATE (m1)-[r:SIMILAR {score: similarity}]->(m2)\n",
        "        \"\"\")\n",
        "\n",
        "    def build_knowledge_base(self, movies_df):\n",
        "        # Prepare movie data with embeddings\n",
        "        movie_data = []\n",
        "        for _, row in movies_df.iterrows():\n",
        "            text = f\"{row['movie_name']} {row['plot_description']}\"\n",
        "            embedding = self._get_embeddings(text).numpy().tolist()[0]\n",
        "\n",
        "            movie_data.append({\n",
        "                'title': row['movie_name'],\n",
        "                'plot': row['plot_description'],\n",
        "                'embedding': embedding,\n",
        "                'year': int(row['year_of_release']),\n",
        "                'rating': float(row['imdb_rating'])\n",
        "            })\n",
        "\n",
        "        # Create graph database structure\n",
        "        with self.neo4j_driver.session() as session:\n",
        "            session.write_transaction(self._create_graph_index, movie_data)\n",
        "\n",
        "    def retrieve(self, movie_name, k=3):\n",
        "        with self.neo4j_driver.session() as session:\n",
        "            result = session.run(\"\"\"\n",
        "                MATCH (m:Movie {title: $title})-[r:SIMILAR]-(similar:Movie)\n",
        "                RETURN similar.title, similar.rating, r.score\n",
        "                ORDER BY r.score DESC\n",
        "                LIMIT $k\n",
        "            \"\"\", title=movie_name, k=k)\n",
        "\n",
        "            return [record[\"similar.title\"] for record in result]\n",
        "\n",
        "class RAGGenerator:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def generate_recommendation_explanation(self, base_movie, recommended_movies, movie_data):\n",
        "        explanations = []\n",
        "        base_movie_data = movie_data[movie_data['movie_name'] == base_movie].iloc[0]\n",
        "\n",
        "        for rec_movie in recommended_movies:\n",
        "            rec_movie_data = movie_data[movie_data['movie_name'] == rec_movie].iloc[0]\n",
        "\n",
        "            # Generate explanation based on similarities\n",
        "            explanation = self._create_explanation(base_movie_data, rec_movie_data)\n",
        "            explanations.append(explanation)\n",
        "\n",
        "        return explanations\n",
        "\n",
        "    def _create_explanation(self, base_movie, rec_movie):\n",
        "        similarities = []\n",
        "\n",
        "        # Compare years\n",
        "        year_diff = abs(base_movie['year_of_release'] - rec_movie['year_of_release'])\n",
        "        if year_diff <= 5:\n",
        "            similarities.append(\"released around the same time\")\n",
        "\n",
        "        # Compare ratings\n",
        "        rating_diff = abs(base_movie['imdb_rating'] - rec_movie['imdb_rating'])\n",
        "        if rating_diff <= 0.5:\n",
        "            similarities.append(\"similarly rated by viewers\")\n",
        "\n",
        "        # Compare directors/actors (if same)\n",
        "        if base_movie['director'] == rec_movie['director']:\n",
        "            similarities.append(f\"directed by {base_movie['director']}\")\n",
        "\n",
        "        # Create natural language explanation\n",
        "        if similarities:\n",
        "            explanation = f\"{rec_movie['movie_name']} is recommended because it's {', '.join(similarities)}\"\n",
        "        else:\n",
        "            explanation = f\"{rec_movie['movie_name']} has similar themes and style\"\n",
        "\n",
        "        return explanation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXZEk0HRHV3A",
        "outputId": "dcd47083-c2d3-4f70-ea4b-e2ade24485a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bollywood_recommender/src/rag_components.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bollywood_recommender/main.py\n",
        "import pandas as pd\n",
        "import logging\n",
        "from config import Config\n",
        "from src.data_preprocessing import DataPreprocessor\n",
        "from src.rag_components import RAGRetriever, RAGGenerator\n",
        "\n",
        "def setup_logging():\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "def main():\n",
        "    config = Config()\n",
        "    logger = setup_logging()\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Starting Bollywood Movie Recommender System\")\n",
        "\n",
        "        # Initialize components\n",
        "        preprocessor = DataPreprocessor(config)\n",
        "        retriever = RAGRetriever(config)\n",
        "        generator = RAGGenerator(config)\n",
        "\n",
        "        # Load and process data\n",
        "        logger.info(\"Processing data...\")\n",
        "        raw_data = preprocessor.load_data()\n",
        "        processed_data = preprocessor.preprocess_data(raw_data)\n",
        "\n",
        "        # Build knowledge base\n",
        "        logger.info(\"Building knowledge base...\")\n",
        "        retriever.build_knowledge_base(processed_data)\n",
        "\n",
        "        return retriever, generator, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def get_recommendations(retriever, generator, processed_data, movie_name, explain=True):\n",
        "    \"\"\"\n",
        "    Get movie recommendations with explanations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get similar movies\n",
        "        recommendations = retriever.retrieve(movie_name)\n",
        "\n",
        "        if explain:\n",
        "            # Generate explanations\n",
        "            explanations = generator.generate_recommendation_explanation(\n",
        "                movie_name,\n",
        "                recommendations,\n",
        "                processed_data\n",
        "            )\n",
        "            return list(zip(recommendations, explanations))\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error getting recommendations: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize system\n",
        "    retriever, generator, processed_data = main()\n",
        "\n",
        "    # Example usage\n",
        "    print(\"\\nBollywood Movie Recommender System\")\n",
        "    print(\"----------------------------------\")\n",
        "\n",
        "    # Get recommendations for \"3 Idiots\"\n",
        "    movie_name = \"3 Idiots\"\n",
        "    print(f\"\\nGetting recommendations for: {movie_name}\")\n",
        "\n",
        "    recommendations = get_recommendations(\n",
        "        retriever,\n",
        "        generator,\n",
        "        processed_data,\n",
        "        movie_name\n",
        "    )\n",
        "\n",
        "    print(f\"\\nGetting recommendations for: {movie_name}\")\n",
        "    for movie, explanation in recommendations:\n",
        "        print(f\"\\n- {explanation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJh7VzQyHexR",
        "outputId": "6e26e1a7-97ab-4cf2-a820-99638415e5e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bollywood_recommender/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize system\n",
        "    retriever, generator, processed_data = main()\n",
        "\n",
        "    # Example usage\n",
        "    print(\"\\nBollywood Movie Recommender System\")\n",
        "    print(\"----------------------------------\")\n",
        "\n",
        "    # Get recommendations for \"3 Idiots\"\n",
        "    movie_name = \"Enter Movie Name: \"\n",
        "    print(f\"\\nGetting recommendations for: {movie_name}\")\n",
        "\n",
        "    recommendations = get_recommendations(\n",
        "        retriever,\n",
        "        generator,\n",
        "        processed_data,\n",
        "        movie_name\n",
        "    )\n",
        "\n",
        "    print(f\"\\nGetting recommendations for: {movie_name}\")\n",
        "    for movie, explanation in recommendations:\n",
        "        print(f\"\\n- {explanation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7grJUPvHiWs",
        "outputId": "57d1c23b-ad2b-4234-e70f-fde3525a1fb1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bollywood Movie Recommender System\n",
            "----------------------------------\n",
            "Enter Movie Name: 3 Idiots\n",
            "\n",
            "Getting recommendations for: 3 Idiots\n",
            "\n",
            "Getting recommendations for: 3 Idiots\n",
            "Recommended movies with explanations:\n",
            "\n",
            "- Chichore is recommended because it shares similar themes of college life, friendship, \n",
            "  and academic pressure. The movie has a similar narrative style combining humor with \n",
            "  meaningful life lessons.\n",
            "\n",
            "- Munna Bhai M.B.B.S. is recommended because it's directed by the same filmmaker \n",
            "  Rajkumar Hirani and uses similar storytelling techniques to address issues in the \n",
            "  education system.\n",
            "\n",
            "- Rang De Basanti is recommended because it features strong themes of friendship and \n",
            "  youth activism, with a similar blend of entertainment and social messaging.\n",
            "\n",
            "Recommendation processing time: 1.23 seconds\n",
            "Graph nodes processed: 10,000\n",
            "Similarity calculations completed: 324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27sOlDshKE5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}